# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from itertools import combinations
from scipy.stats import boxcox
from sklearn.preprocessing import PowerTransformer, StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor
from scipy.stats import loguniform
import warnings
warnings.filterwarnings('ignore')

# =========================
# ğŸ”§ åƒæ•¸ï¼ˆä¸åšé ç¯©ï¼›åˆ—èˆ‰æ‰€æœ‰åˆæ³•çµ„åˆï¼›RBF å°ˆå±¬ï¼‰
# =========================
DATA_PATH   = r'D:\OneDrive\æ¡Œé¢\æ–°æ¨¡å‹\0805.xlsx'
SHEET_NAME  = 'Sheet1'
TEST_SIZE   = 0.30
RANDOM_SEED = 42

# æšèˆ‰æ§åˆ¶ï¼šä»¥ã€Œå–®ä½ã€æšèˆ‰ï¼ˆå…©çµ„æ–¹å‘ç‚ºæˆå°å–®ä½ï¼Œå…¶é¤˜å–®ä¸€å–®ä½ï¼‰
# è‹¥æƒ³çœŸãƒ»å…¨æšèˆ‰ï¼Œè«‹å°‡ MAX_UNITS=Noneï¼›è‹¥è¦æ§ç®—é‡ï¼Œå¯è¨­ 6~9
MAX_UNITS = None          # None = ä½¿ç”¨ 1..å…¨éƒ¨å–®ä½ çš„æ‰€æœ‰çµ„åˆ
EXCLUDE_FEATURES = []     # è¦æ’é™¤çš„ç‰¹å¾µåç¨±ï¼ˆåŒ…å«åœ¨æˆå°å–®ä½ä¸­çš„ä»»ä¸€æˆå“¡å‰‡æ•´å€‹å–®ä½æ’é™¤ï¼‰
MUST_INCLUDE     = []     # å¿…å«çš„ç‰¹å¾µåç¨±ï¼ˆè‹¥å±¬æ–¼æˆå°å–®ä½ï¼Œå‰‡æ•´å€‹å–®ä½å¿…å«ï¼‰

# äº¤å‰é©—è­‰èˆ‡è©•åˆ†
N_SPLITS = 5
SCORING  = 'neg_root_mean_squared_error'
N_ITER_RANDOM = 120  # éš¨æ©Ÿç²—æœæ¬¡æ•¸ï¼ˆå¯é™åˆ° 60 ä»¥åŠ é€Ÿï¼‰

# =========================
# 1) è®€è³‡æ–™ + å‰è™•ç†
# =========================
df = pd.read_excel(DATA_PATH, sheet_name=SHEET_NAME)

# Yeoâ€“Johnson
for col in ['æ³¢é«˜', 'é™é›¨', 'æš´é¢¨åŠå¾‘']:
    pt = PowerTransformer(method='yeo-johnson', standardize=False)
    df[f'{col}_YJ'] = pt.fit_transform(df[[col]])

# æ½®ä½å¹³æ–¹
df['æ½®ä½_BC2'] = (df['æ½®ä½'] - df['æ½®ä½'].min() + 1e-6) ** 2

# æ³¢èƒ½/åŠŸç‡ log1pï¼ˆå¹³ç§»åˆ° >=0ï¼‰
for col in ['æ³¢èƒ½', 'åŠŸç‡']:
    df[f'{col}_log1p'] = np.log1p(df[col] - df[col].min() + 1e-6)

# å°–å³°é€±æœŸ Boxâ€“Coxï¼ˆlambda 2.4217ï¼‰
x = df['å°–å³°é€±æœŸ']
x_pos = x - x.min() + 1e-6
df['å°–å³°é€±æœŸ_BC'] = boxcox(x_pos, 2.4217)

# å€™é¸ç‰¹å¾µï¼ˆä¾›ç´¢å¼•èˆ‡å»ºæ¨¡ä½¿ç”¨ï¼‰
candidate_feats = [
    'é¢¨é€Ÿ', 'æ°£å£“',
    'wind_dir_sin','wind_dir_cos',
    'wave_dir_sin','wave_dir_cos',
    'æ³¢é«˜_YJ', 'é™é›¨_YJ', 'æš´é¢¨åŠå¾‘_YJ',
    'æ½®ä½_BC2',
    'æ³¢èƒ½_log1p', 'åŠŸç‡_log1p',
    'å°–å³°é€±æœŸ_BC'
]

X_all = df[candidate_feats].values
y_all = df['y'].values

X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(
    X_all, y_all, test_size=TEST_SIZE, random_state=RANDOM_SEED
)

# =========================
# 2) å»ºç«‹ã€Œå–®ä½ã€èˆ‡å·¥å…·
# =========================
# å°‡æ–¹å‘è¦–ç‚ºæˆå°å–®ä½ï¼›å…¶ä»–çš†å–®ä¸€å–®ä½
PAIR_UNITS = [
    ('wind_dir_sin','wind_dir_cos'),
    ('wave_dir_sin','wave_dir_cos'),
]
SINGLE_UNITS = [
    ('é¢¨é€Ÿ',), ('æ°£å£“',),
    ('æ³¢é«˜_YJ',), ('é™é›¨_YJ',), ('æš´é¢¨åŠå¾‘_YJ',),
    ('æ½®ä½_BC2',),
    ('æ³¢èƒ½_log1p',), ('åŠŸç‡_log1p',),
    ('å°–å³°é€±æœŸ_BC',)
]
ALL_UNITS = PAIR_UNITS + SINGLE_UNITS  # å…± 11 å€‹ã€Œå–®ä½ã€

def feature_in_unit(u, f):
    return f in u

# ä¾ EXCLUDE_FEATURES ç§»é™¤æ•´å€‹å–®ä½
filtered_units = []
for unit in ALL_UNITS:
    if any(f in EXCLUDE_FEATURES for f in unit):
        continue
    filtered_units.append(unit)

# ä¾ MUST_INCLUDE æ¨™è¨˜å¿…å«å–®ä½
required_units = []
for unit in filtered_units:
    if any(f in MUST_INCLUDE for f in unit):
        required_units.append(unit)
# å»é‡
required_units = list(dict.fromkeys(required_units))

# æ–¹ä¾¿æŠŠã€Œå–®ä½çµ„åˆã€å±•é–‹ç‚ºç‰¹å¾µæ¸…å–®
def flatten_features(unit_combo):
    feats = []
    for u in unit_combo:
        feats.extend(list(u))
    return feats

# RBF æ¨¡å‹èˆ‡å…©éšæ®µæœå°‹è¨­å®š
cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)

base_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('svr', SVR())
])
base_model = TransformedTargetRegressor(
    regressor=base_pipe,
    transformer=StandardScaler()
)

# éš¨æ©Ÿç²—æœç©ºé–“ï¼ˆRBFï¼‰
param_distributions = [
    # é€£çºŒ gamma
    {
        'regressor__svr__kernel': ['rbf'],
        'regressor__svr__C': loguniform(1e-3, 1e3),
        'regressor__svr__epsilon': loguniform(1e-4, 2.0),
        'regressor__svr__gamma': loguniform(1e-4, 1e1)
    },
    # å…§å»º gammaï¼šscale / auto
    {
        'regressor__svr__kernel': ['rbf'],
        'regressor__svr__C': loguniform(1e-3, 1e3),
        'regressor__svr__epsilon': loguniform(1e-4, 2.0),
        'regressor__svr__gamma': ['scale', 'auto']
    },
]

def stage2_grid_from_best_rbf(rnd_best_estimator):
    svr_step = rnd_best_estimator.regressor_.named_steps['svr']
    C0 = float(svr_step.C)
    eps0 = float(svr_step.epsilon)

    # gamma åŸºæº–
    g_used = getattr(svr_step, "_gamma", svr_step.gamma)
    if isinstance(g_used, str):
        try:
            nfeat = svr_step.n_features_in_
            g0 = 1.0 / max(1, nfeat)
        except Exception:
            g0 = 0.1
    else:
        g0 = float(g_used)

    C_grid = sorted({max(1e-6, C0 * f) for f in (0.2, 0.5, 1.0, 2.0, 5.0)})
    eps_grid = sorted({max(1e-6, eps0 * f) for f in (0.5, 0.8, 1.0, 1.2, 1.5)})
    gamma_grid_numeric = sorted({max(1e-8, g0 * f) for f in (0.25, 0.5, 1.0, 2.0, 4.0)})

    gamma_list = list(gamma_grid_numeric)
    if isinstance(svr_step.gamma, str):
        gamma_list += [svr_step.gamma]  # ä¿ç•™ 'scale'/'auto'

    return {
        'regressor__svr__kernel': ['rbf'],
        'regressor__svr__C': C_grid,
        'regressor__svr__epsilon': eps_grid,
        'regressor__svr__gamma': gamma_list
    }

# =========================
# 3) åˆ—èˆ‰æ‰€æœ‰åˆæ³•ã€Œå–®ä½çµ„åˆã€ + å…©éšæ®µæœå°‹ï¼ˆRBFï¼‰
# =========================
if MAX_UNITS is None:
    r_range = range(1, len(filtered_units) + 1)
else:
    r_range = range(1, min(MAX_UNITS, len(filtered_units)) + 1)

results = []
print("ä¸åšç‰¹å¾µé ç¯©ï¼Œå°‡ä»¥ã€å–®ä½ã€åˆ—èˆ‰æ‰€æœ‰åˆæ³•çµ„åˆï¼ˆæ–¹å‘æˆå°ï¼‰ã€‚")
print(f"å¯ç”¨å–®ä½æ•¸é‡ï¼š{len(filtered_units)}ï¼Œå¿…å«å–®ä½æ•¸é‡ï¼š{len(required_units)}")
print("=" * 60)

for r in r_range:
    if r < len(required_units):
        continue
    print(f"â–¶ è™•ç†å–®ä½çµ„åˆå¤§å° r = {r} ...")

    for unit_combo in combinations(filtered_units, r):
        # å¿…å«å–®ä½ç´„æŸ
        if any(req not in unit_combo for req in required_units):
            continue

        # å±•é–‹ç‚ºç‰¹å¾µæ¸…å–®èˆ‡ç´¢å¼•
        feats = flatten_features(unit_combo)
        idx = [candidate_feats.index(c) for c in feats]

        X_tr = X_train_all[:, idx]
        X_te = X_test_all[:, idx]
        y_tr = y_train_all
        y_te = y_test_all

        # Stage 1: éš¨æ©Ÿç²—æœï¼ˆRBFï¼‰
        rnd = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_distributions,
            n_iter=N_ITER_RANDOM,
            cv=cv,
            scoring=SCORING,
            n_jobs=-1,
            random_state=RANDOM_SEED,
            verbose=0
        )
        try:
            rnd.fit(X_tr, y_tr)
        except Exception as e:
            print(f"  âš  RandomizedSearch å¤±æ•—ï¼Œå–®ä½çµ„åˆ {unit_combo}: {e}")
            continue

        # Stage 2: ä»¥æœ€ä½³é»ç‚ºä¸­å¿ƒå¾®èª¿ï¼ˆRBFï¼‰
        grid = stage2_grid_from_best_rbf(rnd.best_estimator_)
        gcv = GridSearchCV(
            estimator=rnd.best_estimator_,
            param_grid=grid,
            cv=cv,
            scoring=SCORING,
            n_jobs=-1,
            verbose=0
        )
        try:
            gcv.fit(X_tr, y_tr)
        except Exception as e:
            print(f"  âš  GridSearch å¤±æ•—ï¼Œå–®ä½çµ„åˆ {unit_combo}: {e}")
            continue

        # è©•ä¼°
        y_pred = gcv.best_estimator_.predict(X_te)
        mse_test = mean_squared_error(y_te, y_pred)
        rmse_test = float(np.sqrt(mse_test))
        r2_test = float(r2_score(y_te, y_pred))

        y_tr_pred = gcv.best_estimator_.predict(X_tr)
        r2_train = float(r2_score(y_tr, y_tr_pred))

        results.append({
            'Features': tuple(feats),
            'Num': len(feats),
            'Units': unit_combo,
            'Best_Kernel': 'rbf',
            'Best_Params': gcv.best_params_,
            'CV_RMSE': -gcv.best_score_,
            'R2_train': r2_train,
            'R2_test': r2_test,
            'MSE_test': mse_test,
            'RMSE_test': rmse_test
        })

# =========================
# 4) çµæœæ•´ç†
# =========================
print("\næœå°‹å®Œæˆï¼Œæ•´ç†çµæœ...")
res_df = pd.DataFrame(results)

if len(res_df) == 0:
    print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„çµæœ")
else:
    filtered = (
        res_df.query("R2_train > 0 and R2_test > 0.3")
              .sort_values('R2_test', ascending=False)
              .head(10)
    )
    if len(filtered) == 0:
        print("ç¬¦åˆé–€æª»çš„çµæœç‚ºç©ºï¼Œæ”¹åˆ—å‡ºæ•´é«”å‰ 10ï¼š")
        filtered = res_df.sort_values('R2_test', ascending=False).head(10)

    # å‹å–„åˆ—å°
    def fmt_feats(tup):
        return '(' + ', '.join(f"'{f}'" for f in tup) + ')'
    filtered_print = filtered.copy()
    filtered_print['Features'] = filtered_print['Features'].apply(fmt_feats)

    print("\n" + "="*100)
    print("æœ€ä½³çµæœ (Top 10):")
    print("="*100)
    print(filtered_print[['Features', 'Num', 'Best_Kernel', 'CV_RMSE', 'R2_train', 'R2_test', 'RMSE_test']].to_string(index=False))

    print("\n" + "="*50)
    print("æœ€ä½³æ¨¡å‹è©³ç´°åƒæ•¸:")
    print("="*50)
    best_row = filtered.iloc[0]
    print(f"ç‰¹å¾µ: {fmt_feats(best_row['Features'])}")
    print(f"æ ¸å‡½æ•¸: {best_row['Best_Kernel']}")
    print(f"åƒæ•¸: {best_row['Best_Params']}")
    print(f"è¨“ç·´é›† RÂ²: {best_row['R2_train']:.4f}")
    print(f"æ¸¬è©¦é›† RÂ²: {best_row['R2_test']:.4f}")
    print(f"æ¸¬è©¦é›† RMSE: {best_row['RMSE_test']:.4f}")

print("\nè™•ç†å®Œæˆï¼")
