'''
PCAæª¢æŸ¥(å…¨ç‰¹å¾µ)
'''
# ============================================
# ç·šæ€§å›æ­¸ï¼ˆå…¨éƒ¨ç‰¹å¾µã€Œå…ˆã€æ¨™æº–åŒ–â†’å†åˆ‡åˆ†ï¼‰+ PCA åƒ…åšé‡è¦æ€§æª¢æŸ¥ï¼ˆä¸é€²æ¨¡å‹ï¼‰
# ============================================
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd

from sklearn.preprocessing import PowerTransformer, StandardScaler
from scipy.stats import boxcox, shapiro

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']  # é¡¯ç¤ºä¸­æ–‡
plt.rcParams['axes.unicode_minus'] = False

# ==========================
# 1. è®€å–ä¸¦å‰è™•ç†
# ==========================
df = pd.read_excel(r'D:\OneDrive\æ¡Œé¢\æ–°æ¨¡å‹\0805.xlsx', sheet_name='Sheet1')

# 1.1 æ•¸æ“šè½‰æ›
for col in ['æ³¢é«˜', 'é™é›¨', 'æš´é¢¨åŠå¾‘']:
    pt = PowerTransformer(method='yeo-johnson', standardize=False)
    df[f'{col}_YJ'] = pt.fit_transform(df[[col]])

df['æ½®ä½_BC2'] = (df['æ½®ä½'] - df['æ½®ä½'].min() + 1e-6) ** 2

for col in ['æ³¢èƒ½', 'åŠŸç‡']:
    df[f'{col}_log1p'] = np.log1p(df[col] - df[col].min() + 1e-6)

x = df['å°–å³°é€±æœŸ']
df['å°–å³°é€±æœŸ_BC'] = boxcox(x - x.min() + 1e-6, 2.4217)

# 1.2 ç‰¹å¾µèˆ‡ç›®æ¨™
best_features = [
'é¢¨é€Ÿ', 'æ°£å£“', 'wave_dir_sin', 'wave_dir_cos', 'æ½®ä½_BC2', 
'é™é›¨_YJ', 'æ³¢èƒ½_log1p', 'åŠŸç‡_log1p', 'å°–å³°é€±æœŸ_BC', 
'wind_dir_sin', 'wind_dir_cos','æ³¢é«˜_YJ', 'æš´é¢¨åŠå¾‘_YJ'
]
X = df[best_features].copy()
y = df['y'].copy()

# ==========================
# 2. ã€Œå…ˆã€å…¨è³‡æ–™æ¨™æº–åŒ– â†’ å†åˆ‡åˆ†
# ==========================
scaler = StandardScaler().fit(X)  # â˜… æ”¹å‹•é»ï¼šå°ã€Œå…¨éƒ¨ Xã€fit
X_std = pd.DataFrame(scaler.transform(X), columns=best_features, index=X.index)

# å†åˆ‡åˆ†ï¼ˆåˆ‡çš„æ˜¯æ¨™æº–åŒ–å¾Œçš„ X_stdï¼‰
X_train_std, X_test_std, y_train, y_test = train_test_split(
    X_std, y, test_size=0.3, random_state=42
)

# ==========================
# 3. VIFï¼ˆä»¥ã€Œè¨“ç·´é›†ï¼ˆå·²æ¨™æº–åŒ–ï¼‰ã€è¨ˆç®—ï¼‰
# ==========================
Xc = sm.add_constant(X_train_std)
vif_tbl = pd.DataFrame({
    'Variable': ['const'] + list(X_train_std.columns),
    'VIF': [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]
})
print("âœ… è¨“ç·´é›†ï¼ˆæ¨™æº–åŒ–å¾Œï¼Œå…ˆå…¨åŸŸæ¨™æº–åŒ–å†åˆ‡åˆ†ï¼‰VIFï¼š")
print(vif_tbl.round(4).to_string(index=False), "\n")

# ==========================
# 4. å»ºç«‹ä¸¦æ“¬åˆæ¨¡å‹ï¼ˆä¸å« PCAï¼‰
# ==========================
lr = LinearRegression()
lr.fit(X_train_std, y_train)

print(f"âœ… æˆªè· Î²0 = {lr.intercept_:.6f}")
for name, coef in zip(best_features, lr.coef_):
    print(f"âœ… ä¿‚æ•¸ Î²({name}) = {coef:.6f}")

# ==========================
# 5. è©•ä¼°èˆ‡è¦–è¦ºåŒ–ï¼ˆæ²¿ç”¨ä½ çš„å‡½å¼ï¼‰
# ==========================
def evaluate_and_plot(model, Xmat, y_true, name):
    y_pred = model.predict(Xmat)
    res = y_true - y_pred

    mse   = mean_squared_error(y_true, y_pred)
    rmse  = np.sqrt(mse)
    mae   = mean_absolute_error(y_true, y_pred)
    r2    = r2_score(y_true, y_pred)

    print(f"\n=== {name} Set ===")
    print(f"MSE:  {mse:.4e}")
    print(f"RMSE: {rmse:,.4f}")
    print(f"MAE:  {mae:,.4f}")
    print(f"RÂ²:   {r2:.6f}")

    plt.figure(figsize=(8,7))
    plt.scatter(y_true, y_pred, alpha=0.7, edgecolor='k', s=150, label='estimate vs True')
    mn = min(np.min(y_true), np.min(y_pred))
    mx = max(np.max(y_true), np.max(y_pred))
    plt.plot([mn, mx], [mn, mx], 'r--', linewidth=1.5, label='45Â° Reference')
    plt.xlabel('True y (mÂ³)', fontsize=18)
    plt.ylabel('estimate y (mÂ³)', fontsize=18)
    plt.title(f'{name} Set', fontsize=20)
    plt.xticks(fontsize=16); plt.yticks(fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=16)
    plt.tight_layout()
    plt.show()

    # Shapiro-Wilk æ­£æ…‹æ€§æª¢å®šï¼ˆæ®˜å·®ï¼‰
    res = np.asarray(res).ravel()
    if res.shape[0] > 5000:
        rng = np.random.default_rng(seed=42)
        idx = rng.choice(res.shape[0], 5000, replace=False)
        res_for_test = res[idx]
    else:
        res_for_test = res
    stat, p = shapiro(res_for_test)
    print(f"Shapiro-Wilk ({name} Residuals): Statistic={stat:.4f}, p-value={p:.4f}")
    print("â¡ï¸ æ®˜å·®è¿‘ä¼¼æ­£æ…‹åˆ†å¸ƒ\n" if p > 0.05 else "â¡ï¸ æ®˜å·®ä¸ç¬¦åˆæ­£æ…‹åˆ†å¸ƒ\n")

def plot_scatter_true_vs_pred_topk(y_true, y_pred, title, top_k=5, fmt="{:,.0f}"):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    plt.figure(figsize=(7, 7))
    plt.scatter(y_true, y_pred, alpha=0.7, s=150, edgecolor='k', label='estimate vs True')

    lims = [np.min([y_true.min(), y_pred.min()]),
            np.max([y_true.max(), y_pred.max()])]
    plt.plot(lims, lims, 'r--', lw=2, label='45Â° Reference')

    resid = np.abs(y_true - y_pred)
    idx = np.argsort(resid)[-top_k:]

    for i in idx:
        try:
            true_str = fmt.format(y_true[i])
        except Exception:
            true_str = f"{y_true[i]:.2f}"
        delta_val = resid[i]
        plt.annotate(
            f"{true_str}\nÎ”={delta_val:,.0f}",
            xy=(y_true[i], y_pred[i]),
            xytext=(8, -8), textcoords="offset points",
            fontsize=14,
            bbox=dict(boxstyle="round,pad=0.15", fc="white", ec="red", lw=1, alpha=0.7)
        )
        plt.scatter([y_true[i]], [y_pred[i]], s=300, facecolors='none', edgecolors='red', linewidths=2)

    plt.xlabel('True y (mÂ³)', fontsize=18)
    plt.ylabel('estimate y (mÂ³)', fontsize=18)
    plt.title(title, fontsize=20)
    plt.xlim(lims); plt.ylim(lims)
    plt.xticks(fontsize=14); plt.yticks(fontsize=14)
    plt.gca().set_aspect('equal', 'box')
    plt.legend(fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# === è©•ä¼°èˆ‡åˆ—è¡¨ ===
y_pred_train = lr.predict(X_train_std)
y_pred_test  = lr.predict(X_test_std)

evaluate_and_plot(lr, X_train_std, y_train, 'Train')
evaluate_and_plot(lr, X_test_std,  y_test,  'Test')

train_pred_tbl = pd.DataFrame({
    'True_y': y_train.reset_index(drop=True),
    'Pred_y': pd.Series(y_pred_train)
})
train_pred_tbl['Residual']  = train_pred_tbl['True_y'] - train_pred_tbl['Pred_y']
train_pred_tbl['Abs_Error'] = train_pred_tbl['Residual'].abs()
print("\n=== Train Set: True vs Pred ===")
print(train_pred_tbl.round(4).to_string(index=False))

test_pred_tbl = pd.DataFrame({
    'True_y': y_test.reset_index(drop=True),
    'Pred_y': pd.Series(y_pred_test)
})
test_pred_tbl['Residual']  = test_pred_tbl['True_y'] - test_pred_tbl['Pred_y']
test_pred_tbl['Abs_Error'] = test_pred_tbl['Residual'].abs()
print("\n=== Test Set: True vs Pred ===")
print(test_pred_tbl.round(4).to_string(index=False))

plot_scatter_true_vs_pred_topk(y_train, y_pred_train, 'Train Set')
plot_scatter_true_vs_pred_topk(y_test,  y_pred_test,  'Test Set')

# ==========================
# 6. PCAï¼ˆè¨ºæ–·ç”¨ï¼›ä¸é€²æ¨¡å‹ï¼‰
#    â˜… æ”¹å‹•é»ï¼šç”¨ã€ŒX_stdï¼ˆå…¨åŸŸæ¨™æº–åŒ–å¾Œçš„å®Œæ•´è³‡æ–™ï¼‰ã€åš PCA
# ==========================
from sklearn.decomposition import PCA

feature_cols = best_features
pca = PCA(n_components=None, svd_solver='full', random_state=42).fit(X_std)  # â˜… æ”¹å‹•é»

# --- PC1 çš„ç‰¹å¾µå€¼ / è§£é‡‹ç‡ ---
pc1_eigenvalue = pca.explained_variance_[0]          # Î»1
pc1_ratio      = pca.explained_variance_ratio_[0]    # PC1 explained variance ratio

print("\n=== PCAï¼ˆåŸºæ–¼ X_stdï¼Œå…¨åŸŸæ¨™æº–åŒ–å¾Œçš„å®Œæ•´è³‡æ–™ï¼‰===")
print(f"ç‰¹å¾µæ•¸: {X_std.shape[1]}")
print(f"PC1 ç‰¹å¾µå€¼ (Eigenvalue): {pc1_eigenvalue:.6f}")
print(f"PC1 è§£é‡‹è®Šç•°æ¯”ä¾‹: {pc1_ratio*100:.2f}%")

# --- PC1 ç‰¹å¾µå‘é‡ï¼ˆloadingï¼Œä¾ |loading| ç”±å¤§åˆ°å°ï¼‰---
pc1_vector = pca.components_[0]
pc1_loadings = pd.Series(pc1_vector, index=feature_cols, name='PC1_loading') \
                 .sort_values(key=np.abs, ascending=False)
print("\nPC1 ç‰¹å¾µå‘é‡ï¼ˆloadingï¼Œä¾ |loading| ç”±å¤§åˆ°å°ï¼‰ï¼š")
print(pc1_loadings.round(6).to_string())

# --- å…¨éƒ¨ä¸»æˆåˆ†æ‘˜è¦ ---
evr = pca.explained_variance_ratio_
eig_df = pd.DataFrame({
    'PC': [f'PC{i+1}' for i in range(len(evr))],
    'Eigenvalue': pca.explained_variance_,
    'Explained_%': evr * 100,
    'Cumulative_%': np.cumsum(evr) * 100
})
print("\nå…¨éƒ¨ä¸»æˆåˆ†æ‘˜è¦:")
print(eig_df.round(4).to_string(index=False))

# --- å…¨éƒ¨ç‰¹å¾µå‘é‡ï¼ˆloadings çŸ©é™£ï¼‰---
loadings_df = pd.DataFrame(
    pca.components_.T, index=feature_cols,
    columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])]
)
print("\nå…¨éƒ¨ç‰¹å¾µå‘é‡:")
print(loadings_df.round(6).to_string())

# ï¼ˆå¯é¸ï¼‰è‹¥æƒ³åŠ ç¸½å„ PC çš„åŠ æ¬Šå¹³æ–¹ loading ä½œç‚ºæ•´é«”ç‰¹å¾µé‡è¦æ€§ï¼š
# importance_i = Î£_j ( components[j,i]^2 * evr[j] )
# pca_importance = pd.Series((pca.components_**2).T @ evr,
#                            index=feature_cols, name='PCA_importance_%') * 100
# print("\nğŸ” PCA ç‰¹å¾µé‡è¦æ€§ï¼ˆ%ï¼‰ï¼š")
# print(pca_importance.sort_values(ascending=False).round(2).to_string())
